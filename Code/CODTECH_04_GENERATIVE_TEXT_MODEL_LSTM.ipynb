{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation with LSTM (Keras)"
      ],
      "metadata": {
        "id": "LLeI_kaCEA2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "EFUEwPGZEKnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow nltk --quiet"
      ],
      "metadata": {
        "id": "ryEPruVPEW99"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "6MZoHVT3ETgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjnP0gTZEave",
        "outputId": "6d21d765-ad65-4a08-bc91-c5c69b842ec8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 3: Prepare Dataset (Shakespeare - Hamlet)"
      ],
      "metadata": {
        "id": "UXy2dJdWETjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = gutenberg.raw('shakespeare-hamlet.txt').lower()\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences\n",
        "input_sequences = []\n",
        "words = text.split()\n",
        "for i in range(10, len(words)):\n",
        "    seq = words[i-10:i+1]\n",
        "    line = tokenizer.texts_to_sequences([' '.join(seq)])[0]\n",
        "    input_sequences.append(line)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=11, padding='pre'))\n",
        "X = input_sequences[:, :-1]\n",
        "y = tf.keras.utils.to_categorical(input_sequences[:, -1], num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "QmyD9SScEf_v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 4: Build GPU-Compatible LSTM Model"
      ],
      "metadata": {
        "id": "yFtET5_ZEToQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=10))\n",
        "model.add(LSTM(150))  # ✅ This will use CuDNN with T4 GPU\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "IE0MB-qqE3UK",
        "outputId": "0ec0339c-88f4-4b44-949d-46dc06e3f5e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 5: Train Model"
      ],
      "metadata": {
        "id": "6Iy7FzVtETr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mynh988E7KR",
        "outputId": "ff859c6b-84c8-47e3-93c2-ccbadd3f7557"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 41ms/step - accuracy: 0.0318 - loss: 7.0111\n",
            "Epoch 2/5\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 38ms/step - accuracy: 0.0449 - loss: 6.3570\n",
            "Epoch 3/5\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 40ms/step - accuracy: 0.0597 - loss: 6.1350\n",
            "Epoch 4/5\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.0785 - loss: 5.8124\n",
            "Epoch 5/5\n",
            "\u001b[1m925/925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.0967 - loss: 5.4457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7af02228de90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 6: Text Generation Function"
      ],
      "metadata": {
        "id": "VVCE_3xPETvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words=50):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=10, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted_word = tokenizer.index_word.get(np.argmax(predicted), '')\n",
        "        seed_text += ' ' + predicted_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "5qVUVp_hE_zW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-Specific Prompts"
      ],
      "metadata": {
        "id": "oLyQP9JHFPMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM-Specific Prompts\n",
        "(Since LSTM models are trained on Hamlet, keep the tone poetic or dramatic.)\n",
        "\n",
        "\"To be or not to be, that is\"\n",
        "\n",
        "\"O, what a noble mind is here\"\n",
        "\n",
        "\"Thou art more lovely and more\"\n",
        "\n",
        "\"The king hath spoken of\"\n",
        "\n",
        "\"Dost thou hear the winds of\"\n",
        "\n",
        "\"Speak the truth, and thou shalt\"\n",
        "\n",
        "\"My lord, the hour is late, and\"\n",
        "\n",
        "\"Fear not the shadows, for they\"\n",
        "\n",
        "\"He comes bearing news of\"\n",
        "\n",
        "\"By my troth, I saw the ghost of\"\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZXUAEj6FNch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Step 7: User Input"
      ],
      "metadata": {
        "id": "G1LPlsK9ETyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"👉 Enter a topic or starting phrase: \")\n",
        "print(\"\\n📝 Generated Paragraph:\\n\")\n",
        "print(generate_text(user_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLCV7uy4FDNp",
        "outputId": "368682c8-fe7d-42a1-9764-92e5462ee1a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👉 Enter a topic or starting phrase: \"To be or not to be, that is\"\n",
            "\n",
            "📝 Generated Paragraph:\n",
            "\n",
            "\"To be or not to be, that is\" the king and the king is the king ham i am not to the king ham i am not to the king ham i am not to the king ham i am not to the king ham i am not to the king ham i am not to the king\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GeAL2VAdFyeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}