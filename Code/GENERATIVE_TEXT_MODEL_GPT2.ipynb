{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Based Text Generation\n",
        "\n",
        "###### CODTECH 04 GENERATIVE_TEXT_MODEL\n",
        "\n",
        "###### 1.CREATE A TEXT GENERATION MODEL USING GPT OR LSTM TO GENERATE COHERENT PARAGRAPHS ON SPECIFIC TOPICS.\n",
        "###### 2.COMPLETION CERTFICATE WILL BE ISSUED ON Y0UR INTERNSHIP\n",
        "###### 3.DELIVERABLE: A NOTEBOOK DEMONSTRATING GENERATED TEXT BASED ON USER PROMPTS.\n"
      ],
      "metadata": {
        "id": "ootbSMN8CdAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "12-HoZToChIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch --quiet"
      ],
      "metadata": {
        "id": "BDhooCTuCrLh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "79SlELVZCnaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "brRg_Z5SCroT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Load GPT-2 Model and Tokenizer"
      ],
      "metadata": {
        "id": "a_xv5-X2CnfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'  # You can also try: 'distilgpt2', 'gpt2-medium', etc.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEpaz0VDCsgy",
        "outputId": "c9e947be-c2bc-444b-91bd-f136ac0cb0c0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define Text Generation Function"
      ],
      "metadata": {
        "id": "WK_pzxjyCniy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_paragraph(prompt, max_length=200, temperature=0.7, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return textwrap.fill(result, width=100)"
      ],
      "metadata": {
        "id": "viHlCThKCs8y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Ask for User Input"
      ],
      "metadata": {
        "id": "4z2j1dT1Cnmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### \"To be or not to be, that is\"\n",
        "##### \"O, what a noble mind is here\"\n",
        "##### \"Thou art more lovely and more\"\n",
        "##### \"The king hath spoken of\"\n",
        "##### \"Dost thou hear the winds of\"\n",
        "##### \"Speak the truth, and thou shalt\"\n",
        "##### \"My lord, the hour is late, and\"\n",
        "##### \"Fear not the shadows, for they\"\n",
        "##### \"He comes bearing news of\"\n",
        "##### \"By my troth, I saw the ghost of\""
      ],
      "metadata": {
        "id": "1zgZ5DHyDYAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"The future of artificial intelligence is\"\n",
        "\n",
        "\"Once upon a time in a quiet village,\"\n",
        "\n",
        "\"The sun was setting behind the mountains when\"\n",
        "\n",
        "\"In the heart of the city, there lived a man who\"\n",
        "\n",
        "\"Education is the most powerful tool because\"\n",
        "\n",
        "\"On a stormy night, a strange knock came at the door.\"\n",
        "\n",
        "\"The secret to happiness lies in\"\n",
        "\n",
        "\"When technology and nature combine,\"\n",
        "\n",
        "\"I opened the old book and found a letter that said\"\n",
        "\n",
        "\"The spaceship landed on a distant planet and\""
      ],
      "metadata": {
        "id": "iccidwCPDwf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = input(\"ðŸ‘‰ Enter a topic or starting sentence for paragraph generation: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aFfICMlCtYw",
        "outputId": "827dacd8-cf45-4a01-e3b8-2042fc971bb5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ‘‰ Enter a topic or starting sentence for paragraph generation: \"To be or not to be, that is\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Generate and Display the Paragraph"
      ],
      "metadata": {
        "id": "KkxBttD7Cnqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_paragraph(user_prompt)\n",
        "display(Markdown(f\"### âœ… Generated Paragraph:\\n\\n{generated_text}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "JT6kUAq5Ct0e",
        "outputId": "670d4968-fad5-40fa-e71b-927898e37258"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### âœ… Generated Paragraph:\n\n\"To be or not to be, that is\" (from the Greek, meaning \"to be or not to be\"). (It should be noted\nthat this is not the only place where the term \"to be\" is used. \"To be or not to be\" can also be\nused to mean \"to be or not to be.\")  The term \"to be or not to be\" is used more often in the past\ntense of a verb than in the present tense. For example, in the second part of the verb \"to be\", the\nverb \"to be\" is used in the first half of the second part of the verb \"to be\", which means \"to be or\nnot to be\".  There is a third reason why the word \"to be\" is used more frequently in the past tense\nof a verb than in the present tense. In the past tense of a verb, the verb \"to be\" is used in the\nfirst half of the verb \""
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4uybH6rDqjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}